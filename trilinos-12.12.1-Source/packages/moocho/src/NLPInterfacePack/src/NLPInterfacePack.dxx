/** \mainpage NLPInterfacePack: Interfaces to Nonlinear Programs.

This package is a set of interfaces to Nonlinear Programs (NLPs).  Also 
included are some node classes to aid in the implemenation of these interfaces as
well as some testing classes.  The code in this library is divided into two
separate collections (and compiled into two separate libraries).

<ol>

<li> Fundamental interfaces to Nonlinear Programs (NLPs)
(<tt>\ref libNLPInterfacePack_grp "libNLPInterfacePack"</tt>):
This is a library of interfaces to NLPs and minimal extra support code.  The
base level NLP interface is <tt>NLPInterfacePack::NLP</tt>.

<li> Adapters for <tt>TSFCore::Nonlin</tt> problems
(<tt>\ref libNLPInterfacePackTSFCore_grp "libNLPInterfacePackTSFCore"</tt>):
This library contains the class <tt>NLPInterfacePack::NLPTSFCoreNP</tt>.

<li> Auxiliary tools for building NLP and testing NLP subclasses
(<tt>\ref libNLPInterfacePackTools_grp "libNLPInterfacePackTools"</tt>):
This library contains some helper node implementation classes for constructing
concrete NLP subclasses.  Also, classes for testing NLP interfaces are included.
For every NLP interface, there is corresponding testing code that can be used.

<li> Testing utilities
(<tt>\ref libNLPInterfacePackTest_grp "libNLPInterfacePackTest"</tt>):
This library contains generic testing utilities for various NLP interfaces.

<li> Utilites for serial NLPs
(<tt>\ref libNLPInterfacePackSerial_grp "libNLPInterfacePackSerial"</tt>):
This library contains node implementation classes for all serial NLP implementations.

</ol>

ToDo: Finish documentation!

*/
//@{

/** \defgroup libNLPInterfacePack_grp libNLPInterfacePack: Fundamental interfaces to Nonlinear Programs (NLPs).

<ul>

<li> Classes:

  <ul>
  <li> Main interface classes:
    <ul>
    <li><tt>NLPInterfacePack::NLP</tt>
    <li><tt>NLPInterfacePack::NLPObjGrad</tt>
    <li><tt>NLPInterfacePack::NLPFirstOrder</tt>
    <li><tt>NLPInterfacePack::NLPSecondOrder</tt>
    </ul>
  <li> Auxilary interface classes:
    <ul>
    <li><tt>NLPInterfacePack::NLPDirect</tt>
    <li><tt>NLPInterfacePack::NLPVarReductPerm</tt>
    </ul>
  </ul>
.

</ul>

This library contains the fundamental interfaces to NLPs needed to
implement several different types of optimization algorithms.

The formulation of the NLP used by this set of interfaces is as follows:
\verbatim

     min    f(x)
     s.t.   c(x) = 0
            hl <= h(x) <= hu
	        xl <= x <= xu
	where:
	        x    <: R^n
	        c(x) <: R^n -> R^m 
	        h(x) <: R^n -> R^mI 
\endverbatim
This is a very general form of an NLP and allows an optimization algorithm great flexibility in how
it is dealt with.  For example, an interior-point algorithm may add slack variables \c s to convert the
general inequality constraints <tt>hl <= h(x) <= hu</tt> to <tt>h(x) - s == 0</tt> with <tt>hl <= s <= hu</tt>.
However, information as the linearity, or nonlinearity of the governing functions is not available (as it
is in more explicit interfaces like <A HREF="http://www.ampl.com">AMPL</A>).

What is different about these NLP interfaces is that the problem data (vectors, matrices etc.) are
abstracted behind a set of interfaces (see <A HREF="../../AbstractLinAlgPack/html/index.html">AbstractLinAlgPack</A>)
that allow for great flexibility in terms of data structures and computing environments.  For example, vectors
are abstracted behind interfaces and polymorphich <tt>\ref AbstractLinAlgPack::VectorSpace "VectorSpace"</tt> objects
are used to create the needed vector objects from the various vector spaces (i.e. the space for \a x, the space
for \a c(x) and the space for \a h(x) ).  Using these interfaces (as well as some others defined here ???) it is possible
to write optimization algorithms that are independent of nasty details such
as sparse matrix data structures and parallel computing environments.  This allows the implementation an optimization  
to have a much larger impact that what is currently possible.  This is simply not possible in the current generation
of interfaces to mathematical programs.

The most basic NLP interface is called <tt>\ref NLPInterfacePack::NLP "NLP"</tt> (of all the crazy things).  This base
level interface only includes methods for calculating zero order information for the objective and the constraint
functions (i.e \a f(x), \a c(x) and \a h(x)).  Also included in this base interface is access to the other defining
properties of the NLP such as the basic vector spaces, the bound vectors \c xl, \c xu, \c hl and \c hu and initial guess
for the solution.

In order to implement fast and efficient algorithms for NLPs (e.g. SQP, GRG etc.) derivative information is needed for
the governing objective and constraint functions.  However, derivative information is more readily available in some
application areas than other.  To account for this, access to derivative information is partitioned out in a few different
NLP interfaces in an inheritance hierarchy.

In most applications, the simplest type of derivative information to obtain is the gradient of the objective function
\f$ \nabla f(x) \in \Re^n \f$ denoted as \c Gf.  The NLP interface <tt>\ref NLPInterfacePack::NLPObjGrad "NLPObjGrad"</tt>
is defined to compute this gradient vector.

Gradient information for the constraints \a c(x) and \a h(x) can be harder to come by in many applications.  The NLP interface
<tt>\ref NLPInterfacePack::NLPFirstOrder "NLPFirstOrder"</tt> allows clients to create matrix objects that represent
the gradients of the constriants \f$ \nabla c(x) \in \Re^{n \times m} \f$ and \f$ \nabla h(x) \in \Re^{n \times mI} \f$ denoted
as \c Gc and \c Gh respectively.  These matrix objects are hidden behind the abstract interface
<tt>\ref AbstractLinAlgPack::MatrixOp "MatrixOp"</tt> that allows for different data
structure implementations and even parallelism.  Of course for these matrix objects to be of use to an optimization algorithm, 
more information is needed about them (i.e. see <tt>\ref AbstractLinAlgPack::BasisSystem "BasisSystem"</tt>) but this is
beyond the scope of these NLP interfaces.  

In some application areas, it is not even possible to form <tt>\ref AbstractLinAlgPack::MatrixOp "MatrixOp"</tt>
objects for \c Gc and \c Gh.  However, it may be possible to solve for some specific linear systems with sub-Jacobians
formed from these matrices.  The solutions from these specific linear systems are all that is needed by some gradient
based optimization algorithms (i.e. Reduced Space SQP).  For these specialized application areas, the NLP interface
<tt>\ref NLPInterfacePack::NLPDirect "NLPDirect"</tt> is defined.

Finally, perhaps the most difficult type of derivative information to obtain is second derivates in the form of 
symmetric Hessian matrices.  The use of such derivative information can make many optimization algorithms faster
and more robust.  For application areas that can support it, the NLP interface
<tt>\ref NLPInterfacePack::NLPSecondOrder "NLPSecondOrder"</tt> is defined.  This interface is capable of
computing a linear combination of the Hessians of the objective and constaint functions known as the Hessian of
the Lagrangian function (see <tt>\ref NLPInterfacePack::NLP "NLP"</tt> for a definition of the Lagrangian function
for this NLP formulation).  Again, this Hessian matrix object is abstracted behind a matrix interface, in this case
<tt>\ref AbstractLinAlgPack::MatrixSymOp "MatrixSymOp"</tt>, that allows for artibrary implementations
(even using parallelism).

*/

/** \defgroup libNLPInterfacePackTSFCore_grp libNLPInterfacePackTSFCore:  Adapters for <tt>TSFCore::Nonlin</tt> problems

<ul>

<li> Classes:

  <ul>
  <li> Concrete classes:
    <ul>
    <li><tt>NLPInterfacePack::NLPTSFCoreNP</tt>
    </ul>
  </ul>
.

</ul>

ToDo: Finish documentation!

*/

/** \defgroup libNLPInterfacePackTools_grp libNLPInterfacePackTools: Auxiliary tools for building NLP and testing NLP subclasses

<ul>

<li> Classes:

  <ul>
  <li> Concrete utility classes:
    <ul>
    <li><tt>NLPInterfacePack::CalcFiniteDiffProd</tt>
    <li><tt>NLPInterfacePack::NLPBarrier</tt>
    </ul>
  <li> Set options from <tt>OptionsFromStreamPack::OptionsFromStream</tt> object.:
    <ul>
    <li><tt>NLPInterfacePack::CalcFiniteDiffProdSetOptions</tt>
    </ul>
  </ul>
.

</ul>

This library contains a number of classes and subclasses that are
useful in developing and testing concrete NLP implementations.

It is very common to approximate derivatives using finite differences.
In general, finite differencing can lead to inaccurate derivatives
when fine precision is needed and can be expensive to compute.
However, using finite differences to provide some validation for
derivatives computed by an NLP interface is invaluable.  The class
<tt>\ref NLPInterfacePack::CalcFiniteDiffProd
"CalcFiniteDiffProd"</tt> can be used to approximate the products
<tt>Gf'*y</tt>, <tt>Gc'*y</tt> and <tt>Gh'*y</tt> for arbitrary
vectors using one of several different order methods.  Note that
computing each of these products only requires between one to four
extra function evaluations per function.  Therefore, these products
can be computed for even the largest problems.  This class can also be
used to compute the entire dense approximations for \c Gf, \c Gc and
\c Gh by using the unit vectors \c e(i).  Computing these quantities
will be prohibitively expensive for larger problems and is not
generally practical to do so.  However, computing these compete
gradients by finite differences can be very valuable to debug small
NLPs.

The above finite differencing driver classes can be useful in their
own right but they are perhaps the most usefully in developing
automated testing software for NLP interfaces.  There are several NLP
testing classes for various NLP interfaces.

The testing class <tt>\ref NLPInterfacePack::NLPTester "NLPTester"</tt> performs unit testing for the base
<tt>\ref NLPInterfacePack::NLP "NLP"</tt> interface.  No derivatives are checked here but must of the postconditions of all
of the methods are checked.  This class has several options that a user can select to suite the tests and the output to the
task at had (see the class <tt>\ref NLPInterfacePack::NLPTesterSetOptions "NLPTesterSetOptions"</tt>).

The testing class <tt>\ref NLPInterfacePack::NLPDirectTester "NLPDirectTester"</tt> tests the sensitivity
information computed by the <tt>\ref NLPInterfacePack::NLPDirect "NLPDirect"</tt> interface using finite
differences.  There are several options that control the behavior of this testing class and they can be set but the user from
and options file using <tt>\ref NLPInterfacePack::NLPDirectTesterSetOptions "NLPDirectTesterSetOptions"</tt>.

The testing function <tt>\ref NLPInterfacePack::test_nlp_direct "test_nlp_direct()"</tt> puts together
some testing components to perform some though tests of a <tt>\ref NLPInterfacePack::NLPDirect "NLPDirect"</tt>
object.

ToDo: Document testing code for NLPFirstOrder when this code is ready.

*/

/** \defgroup libNLPInterfacePackTest_grp libNLPInterfacePackTest: Generic testing utilities

<ul>

<li> Classes:

  <ul>
  <li> Testing Classes:
    <ul>
    <li><tt>NLPInterfacePack::NLPTester</tt>
    <li><tt>NLPInterfacePack::NLPFirstDerivTester</tt>
    <li><tt>NLPInterfacePack::NLPDirectTester</tt>
    </ul>
  <li> Set options from <tt>OptionsFromStreamPack::OptionsFromStream</tt> object.:
    <ul>
    <li><tt>NLPInterfacePack::NLPTesterSetOptions</tt>
    <li><tt>NLPInterfacePack::NLPFirstDerivTesterSetOptions</tt>
    <li><tt>NLPInterfacePack::NLPDirectTesterSetOptions</tt>
    </ul>
  </ul>
.

<li> Functions:

  <ul>
  <li> Testing functions:
    <ul>
    <li><tt>NLPInterfacePack::test_basis_system()</tt>
    <li><tt>NLPInterfacePack::test_nlp_first_order()</tt>
    <li><tt>NLPInterfacePack::test_nlp_direct()</tt>
    </ul>
  </ul>
.

</ul>

ToDo: Finish documentation!

*/

/** \defgroup libNLPInterfacePackSerial_grp libNLPInterfacePackSerial: Utility subclasses for serial NLPs

<ul>

<li> Classes:

  <ul>
  <li><tt>NLPInterfacePack::NLPSerialPreprocess</tt>
  <li><tt>NLPInterfacePack::NLPSerialPreprocessExplJac</tt>
  </ul>
.

</ul>

ToDo: Finish documentation!

*/

//@}
